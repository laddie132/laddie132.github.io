---
title: "论文分享 - Show and Tell: A Neural Image Caption Generator"
date: 2017-12-25 16:46:00
categories:
- Image Caption

tags: 
- 论文分享
- Image Caption
mathjax: true
---

## 介绍
Image Caption是计算机视觉和自然语言处理相结合的一个任务，作者提出了一种基于神经网络的方法，将用于物体识别的cnn网络和用于机器翻译的lstm网络拼接起来，通过极大化正确描述的似然函数来训练这个网络。发表论文时，pascal数据集上bleu-1得分最高是25分，作者的模型可以到59分，人类水平大约是69分。

该文主要是由于当时机器翻译模型的一些突破性进展，使用rnn模型的话，单个任务驱动，使用一个编码器生成固定长度向量表示，再使用一个解码器生成目标句子。

于是，作者考虑能不能通过替换RNN机器翻译模型中的编码器为CNN网络，实现Image Caption。原因如下：**编码器目的在于抽象特征为固定长度向量，而CNN已被证实具有描述图像特征的功能，因此可以预先训练图片分类模型。**

这里便是作者的模型Neural Image Caption（NIC），使用一个cnn网络作为图像编码器，并在物体识别任务中预训练参数，将最后一个隐含层输入到RNN解码器中。
<img src="/images/nic-model.png" style="zoom:40%" />

这篇论文的主要贡献有：

1. 提出一种端到端的神经网络，可以使用随机梯度下降直接训练
2. 结合了两种出色的神经网络，可以分别在额外数据集上预训练
3. 在公开数据集上得到了出色的效果

## 相关工作

以前Image Caption工作严重依赖规则，主要采用结构化模板生成自然语言；另外，还有很大一部分工作在于评价排序，即从给定的一系列描述中计算图片和文本的关联度，找到最接近的，这种方法不能预先生成描述，只能用于评价描述的准确性。

与该文比较相似的三篇论文：

1. Kiros：利用前馈神经网络，通过图像和当前词预测下一个词
2. Mao：使用RNN网络，与本文最相似，但作者使用了更强大的网络，图片直接输入，测试效果更好
3. Kiros：多模型混合，联合向量定义使用两种独立的方式，图像和文本

## 模型

目标函数：极大化目标句子的似然函数
$$
\theta^* = arg \max_{\theta} \sum_{(I,S))}\log{p(S|I;\theta)}
$$

进一步，应用链式法则对对数似然函数分解，句子S由N个词组成，每一个词对应一个时刻t，故而有：
$$
\log{p(S|I)}=\sum_{t=0}^{N}\log{p(S_t|I,S_0,...,S_{t-1})}
$$
即对于输入图像$I$，输出句子$S$的概率为每一时刻生成对应词$S_{t}$概率的乘积，如果取对数概率，则分解为每一时刻词对数概率的和。

作者发现该公式恰好可以对应到RNN网络结构中，在$t$时刻，需要计算当前词$S_{t}$的概率，则可以将历史词$S_0$到$S_{t-1}$表示为一个确定长度的隐含层神经元向量$h_t$，同时输入图像$x_t$，在$t+1$时刻，隐含层向量被更新：
$$
h_{t+1}=f(h_t,x_t)
$$

### LSTM

上述模型中有一个未知量$f$函数，如果选用RNN网络，则在训练过程中，往往会遇到梯度消失或者梯度爆炸问题。而LSTM网络作为RNN的改进版，很好的规避了这一个缺点。


<img src="/images/lstm-model.png" style="zoom:40%" />

LSTM作为一个记忆单元，内部逻辑结构很简单，可以用以下四个公式来描述：
$$
i_t=\sigma(W_{ix}x_t+W_{im}m_{t-1})\\
f_t=\sigma(W_{fx}x_t+W_{fm}m_{t-1})\\
o_t=\sigma(W_{ox}x_t+W_{om}m_{t-1})\\
c_t=f_t\odot c_{t-1}+i_t\odot h(W_{cx}x_t+W_{cm}m_{t-1})\\
m_t=o_t \odot c_t
$$
其中，$i$代表输入门，$f$代表遗忘门，$o$代表输出门，$c$代表记忆状态，$m$代表输出值

#### 训练

<img src="/images/nic-model2.png" style="zoom:40%" />

这里主要指出了需要注意的几个点：

1. 各个时刻的LSTM单元共享一套参数
2. 单词采用one-hot的表示方法
3. 每一个句子前后都有标志词，表示句子的开始和结束
4. 图像只需要输入一次，作者试验过图像输入到每一时刻的lstm中，结果因噪声很容易过拟合
5. 采用对数损失函数：$L(I,S)=-\sum_{t=1}^{N}\log{p_t(S_t)}$

#### 输出

两种方法：

1. Sampling：直接将前一个词输入下一个时间维
2. BeamSearch方法：第一个时间点，输出top k个候选词，这k个候选词分别输入第二个时间维，得到若干第一个和第二个词组合，从这选择得分top k的，输入第三个时间维，依次迭代

该文采用第二种**BeamSearch**方法

## 实验

首先对原始样本做**bootstrapping**采样，即n次重复抽样，目的在于基于有限少量样本得到真实样本分布。

### 训练细节

论文提到高质量数据集少，标记困难，但还是会遇到过拟合问题，有以下方法：

1. 预训练模型：图像部分直接采用ImageNet模型；lstm部分尝试新闻预料模型训练，效果很差，最后采用随机参数
2. dropout，ensembing models（融合模型）

训练方法采用随机梯度下降法，固定学习速率，无动量项。

对于实验结果，可以查看原文，当时作者取得的效果已经非常出色。

### 思考

1. 迁移学习：即在一个领域训练完成后，可以直接或者少量修改就能应用到另外一个不相关领域中
2. 数据大小：作者测试使用Flickr30k数据集训练比Flickr8k效果提升5分，这里看似是数据驱动的，且有过拟合趋势；但使用mscoco数据（大小大于Flickr30k近5倍），分数降低10
3. 数据标记质量：作者尝试SBU数据集，非人类自主描述产生的句子，测试效果很差
4. 生成多样性：对于上述BeamSearch方法，如果每次只取最佳候选句，则结果80%都会在训练集中，因为样本少，生成的都是范例句；如果每次取top 15的候选句，可以得到很多全新的句子，但是仍然有很好的BLEU得分

## 总结

该论文由于采用了深度神经网络模型，无可例外受到数据集的限制，当数据集规模大的时候，NIC可以显示出很好的效果。另外，作者希望未来可以实现无监督的Image Caption，输入一堆不相关的图片和文本，模型可以自主学习。